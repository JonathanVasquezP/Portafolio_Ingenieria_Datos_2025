{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#M7_AE3_Procesamiento Distribuido de datos  \n",
        "\n",
        "### Explora y transforma datos con RDDs\n",
        "\n",
        "**Contexto:**  \n",
        "Una de las claves para trabajar con datos distribuidos en Spark es conocer cómo manipular RDDs mediante transformaciones. Esta actividad te permitirá practicar con datos reales y aplicar operaciones fundamentales.  \n",
        "\n",
        "**Consigna:**  \n",
        "A partir de un conjunto de datos (puede ser un archivo `.txt` simulado con texto libre o una lista), aplica transformaciones utilizando métodos como `filter`, `map`, `flatMap`, `distinct` y `sortBy` para limpiar y reorganizar la información.  \n",
        "\n",
        "**Paso a paso:**  \n",
        "1. Carga el archivo o lista usando `sc.textFile()` o `sc.parallelize()`.  \n",
        "2. Utiliza `flatMap()` para separar el texto en palabras.  \n",
        "3. Aplica `filter()` para eliminar palabras vacías o con menos de 4 letras.  \n",
        "4. Usa `map()` para crear pares `(palabra, 1)`.  \n",
        "5. Aplica `reduceByKey()` para contar la frecuencia de cada palabra.  \n",
        "6. Ordena los resultados con `sortBy()` y muestra las 5 más frecuentes.  \n",
        "7. Muestra el resultado con `take(5)` o `collect()`.  \n"
      ],
      "metadata": {
        "id": "DgIM-sOVsiPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmG6YqCAsfuA",
        "outputId": "efdc0433-1bdd-4b80-e878-fce403765add"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo datos.txt creado correctamente.\n"
          ]
        }
      ],
      "source": [
        "# Crear un archivo de texto simulado\n",
        "texto = \"\"\"Spark es un motor de procesamiento distribuido diseñado para trabajar con grandes volúmenes de datos.\n",
        "Permite realizar análisis rápidos mediante RDDs y DataFrames.\n",
        "Spark soporta transformaciones y acciones que facilitan la manipulación de datos.\n",
        "El conocimiento de RDDs es fundamental para aplicar filtros, mapeos y agregaciones.\"\"\"\n",
        "\n",
        "# Guardar en un archivo .txt\n",
        "with open(\"datos.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(texto)\n",
        "\n",
        "print(\"Archivo datos.txt creado correctamente.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAL8m4Uxs75c",
        "outputId": "e70e6268-5ff4-4f9b-d1ec-feaa2abc02e6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importar SparkSession y crear la sesión de Spark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "         .appName(\"Colab_PySpark_Basico\")\n",
        "         .getOrCreate())  # Inicializa Spark\n",
        "\n",
        "# Obtener SparkContext desde SparkSession\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(\"Versión de Spark:\", spark.version)\n",
        "print(\"Master:\", sc.master)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf4Acecis_Ow",
        "outputId": "4c3712ac-09af-4b0a-8ee8-3890bddc9785"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versión de Spark: 3.5.1\n",
            "Master: local[*]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.textFile(\"datos.txt\")\n",
        "rdd.take(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9aOcW7muS6_",
        "outputId": "1155e7e2-8cff-4f50-f9c4-72725303caf1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Spark es un motor de procesamiento distribuido diseñado para trabajar con grandes volúmenes de datos.',\n",
              " 'Permite realizar análisis rápidos mediante RDDs y DataFrames.',\n",
              " 'Spark soporta transformaciones y acciones que facilitan la manipulación de datos.',\n",
              " 'El conocimiento de RDDs es fundamental para aplicar filtros, mapeos y agregaciones.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Separar el texto en palabras con flatMap\n",
        "palabras = rdd.flatMap(lambda linea: linea.split())\n",
        "\n",
        "#Filtrar palabras vacías o con menos de 4 letras\n",
        "palabras_filtradas = palabras.filter(lambda palabra: len(palabra) >= 4)\n",
        "\n",
        "#Crear pares (palabra, 1)\n",
        "pares = palabras_filtradas.map(lambda palabra: (palabra.lower().strip(\".,\") , 1))\n",
        "\n",
        "#Contar la frecuencia de cada palabra\n",
        "conteo = pares.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "#Ordenar por frecuencia descendente\n",
        "ordenado = conteo.sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "#Mostrar las 5 palabras más frecuentes\n",
        "print(ordenado.take(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nogX8TgeuUoC",
        "outputId": "19f4ba29-c90c-4d38-99bb-d81cc547c581"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('para', 2), ('spark', 2), ('datos', 2), ('rdds', 2), ('procesamiento', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spark.stop()\n",
        "print(\"Sesión Spark detenida.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mljGXIEJyQG3",
        "outputId": "f8177730-e13c-4348-c627-21fc1a4ce25b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sesión Spark detenida.\n"
          ]
        }
      ]
    }
  ]
}