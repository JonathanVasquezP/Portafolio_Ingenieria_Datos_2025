{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Structured Streaming con procesamiento por ventana y gesti√≥n de eventos tard√≠os\n",
        "\n",
        "##Contexto: üôå\n",
        "Structured Streaming simplifica el procesamiento continuo usando APIs de DataFrame. En esta actividad se simular√° el conteo de eventos por intervalos de tiempo, gestionando eventos tard√≠os con watermark.\n",
        "\n",
        "##Consigna: ‚úçÔ∏è\n",
        "Simula el procesamiento de eventos recibidos por un socket TCP y aplica una ventana temporal de 10 minutos con actualizaci√≥n cada 5 minutos. Usa un watermark de 5 minutos para gestionar eventos atrasados.\n",
        "\n",
        "##Paso a paso:\n",
        "\n",
        "1- Inicia un SparkSession con soporte a Structured Streaming.\n",
        "\n",
        "2- Lee desde un socket TCP con .readStream.format(\"socket\").\n",
        "\n",
        "3- Convierte el texto en palabras y as√≠gnales una marca de tiempo con current_timestamp().\n",
        "\n",
        "4- Aplica groupBy(window(\"timestamp\", \"10 minutes\", \"5 minutes\")) + count().\n",
        "\n",
        "5- Agrega .withWatermark(\"timestamp\", \"5 minutes\").\n",
        "\n",
        "6- Muestra los resultados en consola con .writeStream.outputMode(\"update\").start()."
      ],
      "metadata": {
        "id": "quHxsHGNYHZd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMe_gLZXZHcc",
        "outputId": "d7a77fcc-b166-481a-dcab-4ad5c0382ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Primer muestreo de datos:\n",
            "+------+----+-----+\n",
            "|window|word|count|\n",
            "+------+----+-----+\n",
            "+------+----+-----+\n",
            "\n",
            "Segundo muestreo de datos:\n",
            "+------+----+-----+\n",
            "|window|word|count|\n",
            "+------+----+-----+\n",
            "+------+----+-----+\n",
            "\n",
            "Tercer muestreo de datos:\n",
            "+------------------------------------------+----+-----+\n",
            "|window                                    |word|count|\n",
            "+------------------------------------------+----+-----+\n",
            "|{2025-09-15 22:58:12, 2025-09-15 22:58:17}|0   |36   |\n",
            "|{2025-09-15 22:58:12, 2025-09-15 22:58:17}|1   |36   |\n",
            "|{2025-09-15 22:58:12, 2025-09-15 22:58:17}|2   |36   |\n",
            "|{2025-09-15 22:58:12, 2025-09-15 22:58:17}|3   |35   |\n",
            "|{2025-09-15 22:58:12, 2025-09-15 22:58:17}|4   |35   |\n",
            "|{2025-09-15 22:58:14, 2025-09-15 22:58:19}|0   |76   |\n",
            "|{2025-09-15 22:58:14, 2025-09-15 22:58:19}|1   |76   |\n",
            "|{2025-09-15 22:58:14, 2025-09-15 22:58:19}|2   |76   |\n",
            "|{2025-09-15 22:58:14, 2025-09-15 22:58:19}|3   |75   |\n",
            "|{2025-09-15 22:58:14, 2025-09-15 22:58:19}|4   |75   |\n",
            "|{2025-09-15 22:58:16, 2025-09-15 22:58:21}|0   |100  |\n",
            "|{2025-09-15 22:58:16, 2025-09-15 22:58:21}|1   |100  |\n",
            "|{2025-09-15 22:58:16, 2025-09-15 22:58:21}|2   |100  |\n",
            "|{2025-09-15 22:58:16, 2025-09-15 22:58:21}|3   |100  |\n",
            "|{2025-09-15 22:58:16, 2025-09-15 22:58:21}|4   |100  |\n",
            "|{2025-09-15 22:58:18, 2025-09-15 22:58:23}|0   |100  |\n",
            "|{2025-09-15 22:58:18, 2025-09-15 22:58:23}|1   |100  |\n",
            "|{2025-09-15 22:58:18, 2025-09-15 22:58:23}|2   |100  |\n",
            "|{2025-09-15 22:58:18, 2025-09-15 22:58:23}|3   |100  |\n",
            "|{2025-09-15 22:58:18, 2025-09-15 22:58:23}|4   |100  |\n",
            "+------------------------------------------+----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Instalar PySpark en Colab si no est√°\n",
        "!pip install pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, window\n",
        "import time\n",
        "\n",
        "# Crear SparkSession\n",
        "spark = SparkSession.builder.appName(\"StructuredStreamingColab\").getOrCreate()\n",
        "\n",
        "# Fuente de streaming r√°pida (100 filas por segundo)\n",
        "streaming_input = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 100).load()\n",
        "\n",
        "# Simular categor√≠as con columna word\n",
        "events = streaming_input.withColumn(\"word\", (col(\"value\") % 5).cast(\"string\"))\n",
        "\n",
        "# Aplicar watermark corto\n",
        "events_with_watermark = events.withWatermark(\"timestamp\", \"2 seconds\")\n",
        "\n",
        "# Ventana de 5 segundos con slide de 2 segundos\n",
        "word_counts = events_with_watermark.groupBy(\n",
        "    window(\"timestamp\", \"5 seconds\", \"2 seconds\"), \"word\"\n",
        ").count()\n",
        "\n",
        "# Escribir resultados en memoria\n",
        "query = word_counts.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"tabla_test\") \\\n",
        "    .start()\n",
        "\n",
        "# ‚è≥ Esperar 20s y hacer primer muestreo\n",
        "time.sleep(20)\n",
        "print(\"Primer muestreo de datos:\")\n",
        "spark.sql(\"SELECT * FROM tabla_test ORDER BY window.start, word\").show(truncate=False)\n",
        "\n",
        "# Esperar otros 20s y hacer segundo muestreo\n",
        "time.sleep(20)\n",
        "print(\"Segundo muestreo de datos:\")\n",
        "spark.sql(\"SELECT * FROM tabla_test ORDER BY window.start, word\").show(truncate=False)\n",
        "\n",
        "# Esperar otros 20s y hacer tercer muestreo\n",
        "time.sleep(20)\n",
        "print(\"Tercer muestreo de datos:\")\n",
        "spark.sql(\"SELECT * FROM tabla_test ORDER BY window.start, word\").show(truncate=False)\n",
        "\n",
        "# Detener el stream\n",
        "query.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusi√≥n del experimento de Structured Streaming en Colab:\n",
        "\n",
        "###Generaci√≥n y procesamiento de datos:\n",
        "\n",
        "Se simul√≥ un flujo de eventos usando la fuente rate de Spark, generando 100 filas por segundo.\n",
        "\n",
        "Cada evento se asign√≥ a una categor√≠a (word = value % 5) y se agrup√≥ en ventanas de 5 segundos con slide de 2 segundos.\n",
        "\n",
        "Se aplic√≥ un watermark de 2 segundos para gestionar posibles eventos atrasados.\n",
        "\n",
        "Observaci√≥n de los muestreos:\n",
        "\n",
        "Se realizaron 3 muestreos en la tabla de memoria (tabla_test) para ver c√≥mo se acumulaban los eventos en las ventanas.\n",
        "\n",
        "En el primer muestreo, algunas ventanas aparecieron con pocos o ning√∫n dato porque Spark todav√≠a no hab√≠a completado suficientes batches.\n",
        "\n",
        "En los siguientes muestreos, se pudieron observar claramente los conteos por categor√≠a, mostrando c√≥mo Spark agrupa los eventos en ventanas temporales solapadas.\n",
        "\n",
        "Nota sobre los tiempos:\n",
        "\n",
        "Se utiliz√≥ menos tiempo de espera (20 segundos entre muestreos) que en un escenario real para que el flujo fuera operativo y r√°pido en Colab.\n",
        "\n",
        "En un caso productivo, las ventanas podr√≠an ser de varios minutos y los triggers m√°s espaciados, dependiendo del volumen de datos y la frecuencia deseada de actualizaci√≥n.\n",
        "\n",
        "Aprendizaje principal:\n",
        "\n",
        "Structured Streaming permite procesar datos en tiempo casi real en Colab, aunque hay que ajustar tiempos de ventana, slide y triggers para poder visualizar resultados de manera efectiva en entornos limitados como notebooks."
      ],
      "metadata": {
        "id": "BxakqA8QmR5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-TlbtMCJlIB1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}