{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# M7_AE3_P2: Ejecuta un Job Spark completo con acciones y monitoreo\n",
        "\n",
        "## Contexto\n",
        "Conocer el ciclo de ejecución de un Job Spark (transformaciones → acción → ejecución) permite entender el flujo completo de procesamiento distribuido.\n",
        "\n",
        "## Consigna\n",
        "Simula la ejecución de un job Spark desde la carga de datos hasta la ejecución de una acción como `mean()` o `sum()`.  \n",
        "Identifica cuándo se desencadena realmente el procesamiento y cómo se vería el Job internamente (DAG, Stages y Tasks).\n",
        "\n",
        "## Paso a Paso\n",
        "\n",
        "1. **Carga un RDD numérico** desde una lista o un archivo.\n",
        "2. **Aplica una o más transformaciones** (`filter`, `map`, etc.).\n",
        "3. **Confirma que aún no se ha ejecutado nada** (lazy evaluation).\n",
        "4. **Ejecuta una acción** como `sum()`, `mean()` o `stdev()`.\n",
        "5. **Visualiza cómo Spark organiza el Job**  \n",
        "   - Explicación conceptual  \n",
        "   - Spark UI (si está disponible)\n",
        "6. **Dibuja el DAG del Job** con los pasos aplicados.\n"
      ],
      "metadata": {
        "id": "leFl6guK1KqI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbvbX2ju1I8K",
        "outputId": "ec84e16b-c306-4fd7-e2b2-57e8d0dca3d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD creado (lazy evaluation): ParallelCollectionRDD[10] at readRDDFromFile at PythonRDD.scala:289\n",
            "RDD con transformaciones (lazy): PythonRDD[11] at RDD at PythonRDD.scala:53\n",
            "Resultado de la acción sum(): 60\n",
            "Resultado de la acción mean(): 12.0\n",
            "Resultado de la acción stdev(): 5.656854249492381\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Importar Spark y crear SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"JobSparkEjemplo\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "#Crear un RDD numérico desde una lista\n",
        "numeros = [1,2,3,4,5,6,7,8,9,10]\n",
        "rdd_numeros = sc.parallelize(numeros)\n",
        "print(\"RDD creado (lazy evaluation):\", rdd_numeros)\n",
        "\n",
        "#Aplicar transformaciones (filter y map)\n",
        "rdd_filtrado = rdd_numeros.filter(lambda x: x % 2 == 0)   # Solo números pares\n",
        "rdd_multiplicado = rdd_filtrado.map(lambda x: x * 2)     # Multiplicamos por 2\n",
        "print(\"RDD con transformaciones (lazy):\", rdd_multiplicado)\n",
        "\n",
        "#Confirmar lazy evaluation\n",
        "# Hasta aquí, nada se ejecuta realmente. Solo se crea el plan de ejecución (DAG interno)\n",
        "\n",
        "#Ejecutar acciones\n",
        "suma = rdd_multiplicado.sum()          # Suma\n",
        "media = rdd_multiplicado.mean()        # Media\n",
        "desviacion = rdd_multiplicado.stdev()  # Desviación estándar\n",
        "\n",
        "print(\"Resultado de la acción sum():\", suma)\n",
        "print(\"Resultado de la acción mean():\", media)\n",
        "print(\"Resultado de la acción stdev():\", desviacion)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1- DAG (Directed Acyclic Graph)\n",
        "\n",
        "Spark construye un grafo de dependencias con todas las transformaciones (filter, map).\n",
        "\n",
        "Nada se ejecuta hasta que se llama una acción (sum(), mean(), stdev()).\n",
        "\n",
        "2- Stages\n",
        "\n",
        "El DAG se divide en Stages, cada uno agrupando transformaciones que se pueden ejecutar sin mezclar datos.\n",
        "\n",
        "Ejemplo: Stage 1 → filter + map, Stage 2 → sum/mean/stdev.\n",
        "\n",
        "3- Tasks\n",
        "\n",
        "Cada Stage se divide en Tasks, que se ejecutan en paralelo por partición de datos.\n",
        "\n",
        "4- Lazy Evaluation\n",
        "\n",
        "Las transformaciones son perezosas; solo se ejecutan al llamar una acción."
      ],
      "metadata": {
        "id": "xJYL8IDU4SJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DAG conceptual del Job Spark\n",
        "\n",
        "```text\n",
        "          DAG del Job Spark\n",
        "-------------------------------------------------\n",
        "[RDD original: 1-10]       <-- Datos iniciales\n",
        "           |\n",
        "       Stage 1: Transformaciones\n",
        "   -----------------------------\n",
        "   |                           |\n",
        " filter (pares)            map (*2)\n",
        "   |                           |\n",
        " Task 1                     Task 2   <-- cada partición se procesa en paralelo\n",
        "   |\n",
        "   v\n",
        "Stage 2: Acción (reduce)\n",
        "   -----------------------------\n",
        "   |           |             |\n",
        " sum()       mean()       stdev()   <-- acción dispara ejecución\n",
        "   |           |             |\n",
        " Task 1     Task 1        Task 1    <-- combina resultados parciales\n"
      ],
      "metadata": {
        "id": "NVLLIO8c6D97"
      }
    }
  ]
}